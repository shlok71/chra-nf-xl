{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NeuroForge Advanced Training Pipeline (Continuous Learning)\n",
    "This notebook provides a comprehensive pipeline for training the NeuroForge model with advanced datasets and capabilities, designed for continuous learning and GitHub synchronization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment and Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install transformers datasets accelerate sentencepiece numpy opencv-python scikit-learn pandas\n",
    "!pip install bitsandbytes peft\n",
    "!pip install ipywidgets # For interactive elements\n",
    "!git config --global user.email \"your_email@example.com\" # REPLACE WITH YOUR GITHUB EMAIL\n",
    "!git config --global user.name \"Your GitHub Username\" # REPLACE WITH YOUR GITHUB USERNAME\n",
    "# For pushing to GitHub, you will need to provide your GitHub Personal Access Token (PAT) when prompted.\n",
    "# Ensure your PAT has \"repo\" scope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clone Repository and Load Model/Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "repo_url = \"https://github.com/shlok71/chra-nf-xl.git\"\n",
    "repo_dir = \"/content/chra-nf-xl\"\n",
    "model_save_path = os.path.join(repo_dir, \"neuroforge_trained_model_advanced\")\n",
    "\n",
    "if not os.path.exists(repo_dir):\n",
    "    !git clone {repo_url} {repo_dir}\n",
    "%cd {repo_dir}\n",
    "\n",
    "# Check if a previously trained model exists, otherwise load base GPT2\n",
    "if os.path.exists(model_save_path):\n",
    "    print(\"Loading existing model and tokenizer...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_save_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_save_path)\n",
    "else:\n",
    "    print(\"No existing model found. Loading base GPT2 model...\")\n",
    "    model_name = \"gpt2\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"Model and tokenizer loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation for Continuous Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "import torch\n",
    "\n",
    "class CombinedTextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "\n",
    "def get_training_data(new_texts=None):\n",
    "    all_texts = []\n",
    "\n",
    "    # Add content from pasted_content.txt (if available in repo)\n",
    "    pasted_content_path = os.path.join(repo_dir, \"pasted_content.txt\")\n",
    "    if os.path.exists(pasted_content_path):\n",
    "        with open(pasted_content_path, \"r\") as f:\n",
    "            all_texts.append(f.read())\n",
    "        print(\"pasted_content.txt added to training data.\")\n",
    "\n",
    "    # Load a small subset of TinyStories\n",
    "    try:\n",
    "        tinystories_dataset = load_dataset(\"roneneldan/TinyStories\", split=\"train[:5000]\", cache_dir=\"/content/cache\")\n",
    "        all_texts.extend(tinystories_dataset[\"text\"])\n",
    "        print(\"TinyStories subset loaded.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load TinyStories: {e}\")\n",
    "\n",
    "    # Add new texts from interaction\n",
    "    if new_texts:\n",
    "        all_texts.extend(new_texts)\n",
    "        print(f\"Added {len(new_texts)} new texts from interaction.\")\n",
    "\n",
    "    if not all_texts:\n",
    "        print(\"No text data available for training.\")\n",
    "        return None\n",
    "\n",
    "    max_length = 512\n",
    "    tokenized_texts = tokenizer(all_texts, truncation=True, padding=\"max_length\", max_length=max_length)\n",
    "    return CombinedTextDataset(tokenized_texts)\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Continuous Training Loop with GitHub Sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "import time\n",
    "\n",
    "def train_and_sync(new_texts=None, num_epochs=1, save_interval_steps=100):\n",
    "    train_dataset = get_training_data(new_texts)\n",
    "    if train_dataset is None:\n",
    "        print(\"Skipping training due to no data.\")\n",
    "        return\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./checkpoints\",\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=num_epochs,\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=4,\n",
    "        save_steps=save_interval_steps,\n",
    "        save_total_limit=2,\n",
    "        logging_dir=\"./training_logs\",\n",
    "        logging_steps=10,\n",
    "        report_to=\"none\",\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        push_to_hub=False, # We will handle Git push manually for more control\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    trainer.train()\n",
    "    print(\"Training complete.\")\n",
    "\n",
    "    # Save the trained model and tokenizer\n",
    "    model.save_pretrained(model_save_path)\n",
    "    tokenizer.save_pretrained(model_save_path)\n",
    "    print(f\"Model saved to {model_save_path}\")\n",
    "\n",
    "    # Git operations to sync with GitHub\n",
    "    print(\"Syncing with GitHub...\")\n",
    "    !git add .\n",
    "    !git commit -m \"Update trained model and logs (continuous training)\" || echo \"No changes to commit\"\n",
    "    # Use a token for pushing to avoid interactive prompts. Replace <YOUR_GITHUB_TOKEN>\n",
    "    # with your actual Personal Access Token. Keep it secret!\n",
    "    !git push https://<YOUR_GITHUB_USERNAME>:<YOUR_GITHUB_TOKEN>@github.com/shlok71/chra-nf-xl.git HEAD:neuroforge-training-and-inference\n",
    "    print(\"GitHub sync complete.\")\n",
    "\n",
    "# Example of continuous training (can be run in a loop)\n",
    "# while True:\n",
    "#     train_and_sync(num_epochs=1) # Train for one epoch, then sync\n",
    "#     time.sleep(3600) # Wait for an hour before next training cycle\n",
    "\n",
    "# Initial training run\n",
    "train_and_sync()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Interaction-Based Learning (Conceptual Outline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# This section outlines how interaction-based learning would work.\n",
    "# In a real application, user inputs and model outputs would be captured\n",
    "# and periodically used to fine-tune the model.\n",
    "\n",
    "interaction_history = []\n",
    "\n",
    "def generate_response(prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(inputs.input_ids, max_new_tokens=50, num_return_sequences=1)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "def on_button_click(b):\n",
    "    user_input = text_input.value\n",
    "    if user_input:\n",
    "        model_response = generate_response(user_input)\n",
    "        output_area.append_display_data(HTML(f\"<b>You:</b> {user_input}<br><b>NeuroForge:</b> {model_response}<br>\"))\n",
    "        interaction_history.append(user_input + \" \" + model_response)\n",
    "        text_input.value = \"\"\n",
    "        \n",
    "        # Periodically retrain with new interactions (e.g., every 10 interactions)\n",
    "        if len(interaction_history) % 10 == 0 and len(interaction_history) > 0:\n",
    "            print(\"\nRetraining with recent interactions...\")\n",
    "            train_and_sync(new_texts=interaction_history[-10:]) # Train on last 10 interactions\n",
    "            # Clear interaction history after training if desired\n",
    "            # interaction_history.clear()\n",
    "\n",
    "text_input = widgets.Text(description=\"Your Input:\", layout=widgets.Layout(width=\"80%\"))\n",
    "send_button = widgets.Button(description=\"Send\")\n",
    "output_area = widgets.Output()\n",
    "\n",
    "send_button.on_click(on_button_click)\n",
    "\n",
    "display(text_input, send_button, output_area)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}


